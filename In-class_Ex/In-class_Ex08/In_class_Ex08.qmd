---
title: "In-class Exercise 8"
format:
  html:
    code-fold: True
    code-summary: "Show the code"
    toc: True
    toc-depth: 4
execute:
  eval: True
  echo: True
  warning: False
date: "`r Sys.Date()`"
---

# Geographical Segmentation with Spatially Constrained Clustering Techniques

# Package Loading and Installation


```{r}
pacman::p_load(spdep, sp, tmap, sf, ClustGeo, ggpubr,
               cluster, factoextra, NbClust, heatmaply,
               corrplot, psych, tidyverse,
               GGally)
```

# Data Import
### Geospatial Data

```{r}
#| eval: False
shan_sf <- st_read(dsn = "data/geospatial",
                layer = "myanmar_township_boundaries") %>%
  filter(ST %in% c("Shan (East)", "Shan (North)", "Shan (South)")) %>%
  select(c(2:7))
```

We can also check the class

```{r}
#| eval: False
class(shan_sf)
```
## Importing Aspatial Data

```{r}
ict <- read_csv("data/aspatial/Shan-ICT.csv")
```

## Derivation of New Variables

```{r}
ict_derived <- ict %>%
  mutate(`RADIO_PR` = `Radio`/`Total households`*1000) %>%
  mutate(`TV_PR` = `Television`/`Total households`*1000) %>%
  mutate(`LLPHONE_PR` = `Land line phone`/`Total households`*1000) %>%
  mutate(`MPHONE_PR` = `Mobile phone`/`Total households`*1000) %>%
  mutate(`COMPUTER_PR` = `Computer`/`Total households`*1000) %>%
  mutate(`INTERNET_PR` = `Internet at home`/`Total households`*1000) %>%
  rename(`DT_PCODE` =`District Pcode`,`DT`=`District Name`,
         `TS_PCODE`=`Township Pcode`, `TS`=`Township Name`,
         `TT_HOUSEHOLDS`=`Total households`,
         `RADIO`=`Radio`, `TV`=`Television`, 
         `LLPHONE`=`Land line phone`, `MPHONE`=`Mobile phone`,
         `COMPUTER`=`Computer`, `INTERNET`=`Internet at home`)
```

We can also check the summary statistics of *ict_derived*

```{r}
#| eval: False
summary(ict_derived)
```

# Exploratory Data Analysis (EDA)

## EDA Using Statistical Graphics

We can plot the distribution of the variables (i.e. Number of households with radio) by using appropriate Exploratory Data Analysis (EDA) as shown in the code chunk below.

Histogram is useful to identify the overall distribution of the data values (i.e. left skew, right skew or normal distribution)

```{r}

ggplot(data=ict_derived, 
       aes(x=`RADIO`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")
```

Boxplot is useful to detect if there are outliers.

```{r}
ggplot(data=ict_derived, 
       aes(x=`RADIO`)) +
  geom_boxplot(color="black", 
               fill="light blue")
```

Next, we will also plotting the distribution of the newly derived variables (i.e. Radio penetration rate) by using the code chunk below.

```{r}
ggplot(data=ict_derived, 
       aes(x=`RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")
```

```{r}
ggplot(data=ict_derived, 
       aes(x=`RADIO_PR`)) +
  geom_boxplot(color="black", 
               fill="light blue")
```

What can you observed from the distributions reveal in the histogram and boxplot.

In the figure below, multiple histograms are plotted to reveal the distribution of the selected variables in the *ict_derived* data.frame.

The code chunks below are used to create the data visualisation. They consist of two main parts. First, we will create the individual histograms using the code chunk below.

```{r}
radio <- ggplot(data=ict_derived, 
             aes(x= `RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

tv <- ggplot(data=ict_derived, 
             aes(x= `TV_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

llphone <- ggplot(data=ict_derived, 
             aes(x= `LLPHONE_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

mphone <- ggplot(data=ict_derived, 
             aes(x= `MPHONE_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

computer <- ggplot(data=ict_derived, 
             aes(x= `COMPUTER_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

internet <- ggplot(data=ict_derived, 
             aes(x= `INTERNET_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")
```

Next, the *ggarrange()* function of **ggpubr** package is used to group these histograms together.

```{r}
ggarrange(radio, tv, llphone, mphone, computer, internet, 
          ncol = 3, 
          nrow = 2)
```


```{r}
#| eval: FALSE
shan_sf <- left_join(shan_sf, 
                     ict_derived,
                     by=c("TS_PCODE"="TS_PCODE"))

write_rds(shan_sf, "data/rds/shan_sf.rds")
```

```{r}
shan_sf <- read_rds("data/rds/shan_sf.rds")
```


Note: if you use st_join, both files need to be spatial

```{r}
# correlogram and corplt --> similar to scatter plot but uses elipses
# shape of elipse; rounded = less correlated, narrow = highly correlated
# blue = positive, erd = negative

#returns a matrix
cluster_vars.cor = cor(ict_derived[,12:17])
corrplot.mixed(cluster_vars.cor,
         lower = "ellipse", 
               upper = "number",
               tl.pos = "lt",
               diag = "l",
               tl.col = "black")
```

### Extracting clustering variables

```{r}
cluster_vars <- shan_sf %>%
  st_set_geometry(NULL) %>%
  select("TS.x", "RADIO_PR","TV_PR","LLPHONE_PR", "MPHONE_PR",
         "COMPUTER_PR")

head(cluster_vars, 10)
```


```{r}
shan_ict <- select(cluster_vars, c(2:6))
head(shan_ict, 10)
```
```{r}
write_rds(shan_ict, "data/rds/shan_ict.rds")
```


# Data Standardisation

## Min-Max

```{r}
shan_ict.std <- normalize(shan_ict)
summary(shan_ict.std)
```

## Z-score

```{r}
shan_ict.z <- scale(shan_ict)
describe(shan_ict.z)
```

## Visualising the standardised clustering variables

Beside reviewing the summary statistics of the standardised clustering variables, it is also a good practice to visualise their distribution graphical.

The code chunk below plot the scaled Radio_PR field.

```{r}
r <- ggplot(data=ict_derived, 
             aes(x= `RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue") +
  ggtitle("Raw values without standardisation")

shan_ict_s_df <- as.data.frame(shan_ict.std)
s <- ggplot(data=shan_ict_s_df, 
       aes(x=`RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue") +
  ggtitle("Min-Max Standardisation")

shan_ict_z_df <- as.data.frame(shan_ict.z)
z <- ggplot(data=shan_ict_z_df, 
       aes(x=`RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue") +
  ggtitle("Z-score Standardisation")

ggarrange(r, s, z,
          ncol = 3,
          nrow = 1)
```

```{r}
r <- ggplot(data=ict_derived, 
             aes(x= `RADIO_PR`)) +
  geom_density(color="black",
               fill="light blue") +
  ggtitle("Raw values without standardisation")

shan_ict_s_df <- as.data.frame(shan_ict.std)
s <- ggplot(data=shan_ict_s_df, 
       aes(x=`RADIO_PR`)) +
  geom_density(color="black",
               fill="light blue") +
  ggtitle("Min-Max Standardisation")

shan_ict_z_df <- as.data.frame(shan_ict.z)
z <- ggplot(data=shan_ict_z_df, 
       aes(x=`RADIO_PR`)) +
  geom_density(color="black",
               fill="light blue") +
  ggtitle("Z-score Standardisation")

ggarrange(r, s, z,
          ncol = 3,
          nrow = 1)
```


## Computing proximity matrix
In R, many packages provide functions to calculate distance matrix. We will compute the proximity matrix by using dist() of R.

dist() supports six distance proximity calculations, they are: euclidean, maximum, manhattan, canberra, binary and minkowski. The default is euclidean proximity matrix.

The code chunk below is used to compute the proximity matrix using euclidean method.
```{r}
proxmat <- dist(shan_ict, method = 'euclidean')
```
The code chunk below can then be used to list the content of proxmat for visual inspection.

```{r}
proxmat
```
## Computing hierarchical clustering

In R, there are several packages provide hierarchical clustering function. In this hands-on exercise, hclust() of R stats will be used.

hclust() employed agglomeration method to compute the cluster. Eight clustering algorithms are supported, they are: ward.D, ward.D2, single, complete, average(UPGMA), mcquitty(WPGMA), median(WPGMC) and centroid(UPGMC).

The code chunk below performs hierarchical cluster analysis using ward.D method. The hierarchical clustering output is stored in an object of class hclust which describes the tree produced by the clustering process.
```{r}
hclust_ward <- hclust(proxmat, method = 'ward.D')
```
We can then plot the tree by using plot() of R Graphics as shown in the code chunk below.

```{r}
plot(hclust_ward, cex = 0.6)
```
## Selecting the optimal clustering algorithm
One of the challenge in performing hierarchical clustering is to identify stronger clustering structures. The issue can be solved by using use agnes() function of cluster package. It functions like hclus(), however, with the agnes() function you can also get the agglomerative coefficient, which measures the amount of clustering structure found (values closer to 1 suggest strong clustering structure).

The code chunk below will be used to compute the agglomerative coefficients of all hierarchical clustering algorithms.

```{r}
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

ac <- function(x) {
  agnes(shan_ict, method = x)$ac
}

map_dbl(m, ac)
```

## Optimising Clusters

```{r}
set.seed(12345)
gap_stat <- clusGap(shan_ict, 
                    FUN = hcut, 
                    nstart = 25, 
                    K.max = 10, 
                    B = 50)
# Print the result
print(gap_stat, method = "firstmax")
```

Note: look at gap

Note: Local optimisation, not global. There are multiple optimisation. Choose according to your needs. In this case, the optimal number of clusters is 6.

```{r}
fviz_gap_stat(gap_stat)
```


## Interpreting dendograms

```{r}
plot(hclust_ward, cex = 0.6)
rect.hclust(hclust_ward, 
            k = 6, 
            border = 2:5)
```

## Visually-driven hierarchical clustering analysis
In this section, we will learn how to perform visually-driven hiearchical clustering analysis by using heatmaply package.

With heatmaply, we are able to build both highly interactive cluster heatmap or static cluster heatmap.

### Transforming the data frame into a matrix
The data was loaded into a data frame, but it has to be a data matrix to make your heatmap.

The code chunk below will be used to transform shan_ict data frame into a data matrix.
```{r}
shan_ict_mat <- data.matrix(shan_ict)
```
### Plotting interactive cluster heatmap using heatmaply()

In the code chunk below, the heatmaply() of heatmaply package is used to build an interactive cluster heatmap.
```{r}
heatmaply(normalize(shan_ict_mat),
          Colv=NA,
          dist_method = "euclidean",
          hclust_method = "ward.D",
          seriate = "OLO",
          colors = Blues,
          k_row = 6,
          margins = c(NA,200,60,NA),
          fontsize_row = 4,
          fontsize_col = 5,
          main="Geographic Segmentation of Shan State by ICT indicators",
          xlab = "ICT Indicators",
          ylab = "Townships of Shan State"
          )


```

## Mapping the clusters formed
With closed examination of the dendragram above, we have decided to retain six clusters.

cutree() of R Base will be used in the code chunk below to derive a 6-cluster model.

```{r}
groups <- as.factor(cutree(hclust_ward, k=6))
```

The output is called groups. It is a list object.

In order to visualise the clusters, the groups object need to be appended onto shan_sf simple feature object.

The code chunk below form the join in three steps:

-   the groups list object will be converted into a matrix;
-   cbind() is used to append groups matrix onto shan_sf to produce an output simple feature object called shan_sf_cluster; and
-   rename of dplyr package is used to rename as.matrix.groups field as CLUSTER.

```{r}
shan_sf_cluster <- cbind(shan_sf, as.matrix(groups)) %>%
  rename(`CLUSTER`=`as.matrix.groups.`)
```

Next, qtm() of tmap package is used to plot the choropleth map showing the cluster formed.

```{r}
qtm(shan_sf_cluster, "CLUSTER")
```


# Spatially Constrained Clustering: SKATER approach

No need to do *as_spatial()*

## Computing neighbours list

```{r}
shan.nb <- poly2nb(shan_sf)
summary(shan.nb)
```

```{r}
plot(st_geometry(shan_sf),
     border=grey(.5))

pts = st_coordinates(st_centroid(shan_sf)) # create the centroid circles
plot(shan.nb,
     pts,
     col="blue",
     add= TRUE)
```

Note: can continue using pts since it's saved into environment

All the same as [Hands-on Exercise 9](https://is415-gaa-krislbt.netlify.app/hands-on_ex/hands-on_ex09/hands-on_ex09)


