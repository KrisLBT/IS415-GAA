---
title: "Take Home Exercise 01"
format:
  html:
    code-fold: True
    code-summary: "Show the code"
    toc: True
    toc-depth: 4
execute:
  eval: True
  echo: True
  warning: False
date: "`r Sys.Date()`"
---

# Project Overview

It is important for local and government urban planners to know and understand how, where and when people move. For Singapore, data regarding human mobility has largely been collected and disseminated by the Land Transport Authority (LTA). However, they have only collected and publicised data regarding passenger volume from trains stations and bus stops. While immensely helpful, it still leaves out crucial information about human mobility through other means of transport, making it incomphrensive.

In 2020, Grab released Grab Posisi, a data set regarding the origin and destinations of Grab taxi users in Singapore.

In this take-home exercise, I will be finding the geographical and spatio-temporal distribution of Grab hailing service locations in Singapore.

The distribution will be analysed using the following, which will be displayed on the openstreetmap of Singapore:

-   Traditional Kernel Density Estimation (KDE) layers: to analyse the where origin points tend to cluster

-   Network Kernel Density Estimation (NKDE): to analyse the clusters of origin points, as constrained by the road.

# The Data

| Name                 | Description                                                | Format   | Source                                                                                   |
|-------------|------------------|-------------|-----------------------------|
| MP14_SUBZONE_WEB_PL  | Subzone Boundary of Singapore (2014)                       | .shp     | [data.gov.sg](https://beta.data.gov.sg/datasets/d_d14da225fccf921049ab64238ff473d9/view) |
| Grab Posisi          | Dataframe of Origin and Destination Locations through Grab | .parquet | Provided by professor                                                                    |
| gis_osm_roads_free_1 | Road network of Malaysia, Singapore and Brunei             | .shp     | [geofabrik.de](https://download.geofabrik.de/asia/malaysia-singapore-brunei.html)        |

: Data Sets Used

# Loading the packages

In this section, I will be loading the following packages:

-   maptools

-   sf

-   raster

-   spatstat

-   tmap

-   tidyverse

-   arrow

-   lubridate

-   spNetwork

-   classInt

-   viridis

```{r}
pacman::p_load(maptools, sf, raster, spatstat, tmap, tidyverse, arrow, lubridate, spNetwork, classInt)
```

# General Handling

## Extraction of the Coastal Outline of Mainland Singapore

For the purposes of this analysis, we will also need to extract the Coastal Outline of Singapore. This is to create a boundary for all the points to fall under.

First, we need to read Singapore Master Plan Subzone 2014 to get Singapore's shape overall.

```{r}
mpsz_sf <- st_read(dsn = "../../data/data",
                layer = "MP14_SUBZONE_WEB_PL")
```

We have to check the CRS to ensure that it is in the correct coordinates system.

```{r}
#| eval: False
st_crs(mpsz_sf)
```

Since it is in the wrong CRS (WGS84), we have to correct it to SVY21.

```{r}
mpsz_sf <- st_transform(mpsz_sf, 3414)
```

Now, we can plot the island.

```{r}
#| eval: False
plot(mpsz_sf)
```

We notice that the planning subzone includes areas where Grab does not serve due to lack of bridges. Hence, any roads in unavailable areas would either not be useful or actually distort the network analysis.

Most of the areas unserved by Grab are the outer islands. For this, we can identify which of the outer islands to exclude from our analysis.

```{r}
#| eval: False
outer_islands<-  filter(mpsz_sf, grepl('ISLAND', PLN_AREA_N))
plot(outer_islands["SUBZONE_N"])
```

However, we realise that not all the islands are unavailable to Grab drivers. Grab drivers are able to pick up and drop off at [Sentosa](https://www.sentosa.gov.sg/files/resources/news/20191121_media_release_sdc_grab_partnership.pdf).

Since 2019, Grab drivers have been allowed entry to [Jurong Island](https://www.grab.com/sg/driver/transport/jurong-island-security-pass-enrolment/), provided they have a security pass. However, given that very few drivers are allowed entry and the fact that this region is generally inaccessible to the public, we will be excluding them.

Hence, the islands we need to exclude are:

-   Semakau
-   Southern Group
-   Sudong
-   North-eastern Islands
-   Jurong Island and Bukom

```{r}
#| eval: False
serviced_area <- mpsz_sf %>%
  filter(!grepl("SEMAKAU|SOUTHERN GROUP|SUDONG|NORTH-EASTERN ISLANDS|JURONG ISLAND AND BUKOM", SUBZONE_N)) 
plot(serviced_area)
```

Now, we can blend away the boundaries.

```{r}
#| eval: False
sg_sf <- serviced_area %>%
  st_union()
```

Plot the sg_sf and we now see we have the CoastalOutline of Singapore that is served by Grab.

```{r}
#| eval: False
plot(sg_sf)
```

We can save it as the CoastalOutline as a shapefile and read it again

```{r}
#| eval: false
# writing the CoastalOutline
st_write(sg_sf, 'data/CoastalOutline.shp')
```

```{r}
# reading the CoastalOutline
sg_sf <- st_read(dsn='data/',
                 layer = "CoastalOutline")
```

## Handling of the Grab Posisi data

We will also have to handle the Grab Posisi data.

The data is quite large. As such, Grab has divided into 10 parquet files for easier distribution.

### Reading the Grab Posisi Data

I will now be loading the Grab Posisi data

```{r}
#| eval: FALSE
df <- read_parquet(file="../../data/data/GrabPosisi/part-00000.snappy.parquet")
df1 <- read_parquet(file="../../data/data/GrabPosisi/part-00001.snappy.parquet")
df2 <- read_parquet(file="../../data/data/GrabPosisi/part-00002.snappy.parquet")
df3 <- read_parquet(file="../../data/data/GrabPosisi/part-00003.snappy.parquet")
df4 <- read_parquet(file="../../data/data/GrabPosisi/part-00004.snappy.parquet")
df5 <- read_parquet(file="../../data/data/GrabPosisi/part-00005.snappy.parquet")
df6 <- read_parquet(file="../../data/data/GrabPosisi/part-00006.snappy.parquet")
df7 <- read_parquet(file="../../data/data/GrabPosisi/part-00007.snappy.parquet")
df8 <- read_parquet(file="../../data/data/GrabPosisi/part-00008.snappy.parquet")
df9 <- read_parquet(file="../../data/data/GrabPosisi/part-00009.snappy.parquet")
```

### Handling the Grab Posisi data

#### Time stamp

Upon investigating the file, I noticed that there was supposed to be a datetime stamp but it wasn't in that format. As such, the data type needs to be changed.

```{r}
#| eval: FALSE
df$pingtimestamp <- as_datetime(df$pingtimestamp)
df1$pingtimestamp <- as_datetime(df1$pingtimestamp)
df2$pingtimestamp <- as_datetime(df2$pingtimestamp)
df3$pingtimestamp <- as_datetime(df3$pingtimestamp)
df4$pingtimestamp <- as_datetime(df4$pingtimestamp)
df5$pingtimestamp <- as_datetime(df5$pingtimestamp)
df6$pingtimestamp <- as_datetime(df6$pingtimestamp)
df7$pingtimestamp <- as_datetime(df7$pingtimestamp)
df8$pingtimestamp <- as_datetime(df8$pingtimestamp)
df9$pingtimestamp <- as_datetime(df9$pingtimestamp)
```

#### Merging them into 1 data frame

I also noticed that the the same ride might actually have origin and destination points through multiple data frames. As such, we need to merge them before we can extract the origin points.

```{r}
#| eval: False
origin_df_all <- df %>%
  rbind(df1) %>%
  rbind(df2) %>%
  rbind(df3) %>%
  rbind(df4) %>%
  rbind(df5) %>%
  rbind(df6) %>%
  rbind(df7) %>%
  rbind(df8) %>%
  rbind(df9)
```

#### Extraction of Origin

Now, I will be extracting the origin.

The way I did it below is by performing the following steps

-   arrange it according to the trj_id (unique id determining where the person wants to at that point)
-   arranging it according to the time stamp (beginning and end)
-   getting the first row - mutating the data to weekday, start hour and the day

```{r}
#| eval: FALSE
origin_df <- origin_df_all %>%
  group_by(trj_id) %>%
  arrange(pingtimestamp) %>%
  filter(row_number()==1) %>%
  mutate(weekday = wday(pingtimestamp,
                        label=TRUE,
                        abbr=TRUE),
         start_hr= factor(hour(pingtimestamp)),
         day = factor(mday(pingtimestamp)))
```

##### Writing of Origin Data to RDS

Now, I will write the origin data to rds for easier future handling.

```{r}
#| eval: False
write_rds(origin_df, "data/rds/origin_dfs.rds")
```

##### Reading of RDS data

Now, we can clear the data in our environment and just read the rds data we saved.

```{r}
origin_df <- read_rds("data/rds/origin_dfs.rds")
```

## Exploratory Data Analysis and Handling of Origins

In this section, I will be exploring the general distribution of the origin points and handling it for the subsequent sections, which will analyse the data in greater detail.

### Visualising the Frequency Distribution

First, I will plot out the general distribution of the origin points

```{r}
ggplot(data=origin_df,
       aes(x=weekday)) +
  geom_bar()
```

As can be seen, the number of trips daily seem to be fairly equally distributed throughout the week.

### Visualising the Origins as a Point Map Symbol

We can visualise the geospatial distribution of the origin points. First, we need to convert the coordinates to lat-long and transform it to the correct CRS.

```{r}
origins_sf <- st_as_sf(origin_df, coords = c("rawlng", "rawlat"),
                       crs=4326) %>%
  st_transform(crs=3414)
```

Now, we can visualise it using the code chunk below:

```{r}
#| eval: False
tm_shape(mpsz_sf)+
  tm_polygons()+
tm_shape(origins_sf) +
  tm_dots()

```

### Handling

For further analysis, we will need to convert the various data into different formats. These will be explained and performed in the code chunk below

```{r}
#conversion of sg_sf, mpsz_sf and origins_sf to a generic spatial class
sg <- as_Spatial(sg_sf)
mpsz <- as_Spatial(mpsz_sf)
origins <- as_Spatial(origins_sf) 

# Conversion of sg and origins to SpatialPolygon format
sg_sp <- as(sg, "SpatialPolygons")
origins_sp <- as(origins, "SpatialPoints")

# conversion of origins to ppp format
origins_ppp <- as(origins_sp, "ppp")
origins_ppp
```

Now, I will plot origins_ppp.

```{r}
#| eval: False
plot(origins_ppp)
```

We must check for duplicates in the data using the code chunk below:

```{r}
any(duplicated(origins_ppp))
```

As there are no duplicates within the data, we do not have to apply any more transformation to the data.

#### Create Owin Data

Now, we can convert the Coastal Outline to owin data and plot it out.

```{r}
sg_owin <- as(sg_sp, "owin")
plot(sg_owin)
```

#### Combining Events

We will now extract Grab origins within the Singapore CoastalOutline

```{r}
origins_SG_ppp = origins_ppp[sg_owin]

# plotting the distribution of the origins
plot(origins_SG_ppp)
```

# Traditional Kernel Density Estimation Analysis

Now we can begin the traditional kernel density estimation analysis. As mentioned in *Project Overview*, this analysis will enable us to find areas where there is a greater density of origin points. As such, we can find where Grab users tend to book their Grab trips.

## Country Level Analysis

We can begin by trying to find all of the best kernel and bandwidths for the purposes of our analysis. To do this, we need to find the one with the tightest clusters.

In R, we have 4 possible kernel density bandwidths: diggle, ppl, scott and CvL. All of them will be tested below:

```{r}
#| eval: False

# diggle
kde_origins_SG.bw <- density(origins_SG_ppp,
                              sigma=bw.diggle,
                              edge=TRUE,
                            kernel="gaussian") 
# ppl
kde_origins_SG.ppl <- density(origins_SG_ppp, 
                               sigma=bw.ppl, 
                               edge=TRUE,
                               kernel="gaussian")
# CvL
kde_origins_SG.CvL <- density(origins_SG_ppp, 
                               sigma=bw.CvL, 
                               edge=TRUE,
                               kernel="gaussian")
# scott
kde_origins_SG.scott <- density(origins_SG_ppp, 
                               sigma=bw.scott, 
                               edge=TRUE,
                               kernel="gaussian")
```

We may also plot them out:

```{r}
#| eval: False
par(mfrow=c(2,2))
plot(kde_origins_SG.bw)
plot(kde_origins_SG.ppl)
plot(kde_origins_SG.CvL)
plot(kde_origins_SG.scott)
```

Now, we can check for the tightness of these clusters:

```{r}
#| eval: False
# tightness of diggle
bw.diggle(origins_SG_ppp)

# tightness of ppl
bw.ppl(origins_SG_ppp)

# tightness of CvL
bw.CvL(origins_SG_ppp)

# tightness of scott
bw.scott(origins_SG_ppp)

```

We can observe that the smallest sigma value is from diggle, suggesting that it has the tightest clusters of all the bandwidth. Hence, we will continue using diggle for this analysis.

We can also test for different kernels:

```{r}
par(mfrow=c(2,2))
plot(density(origins_SG_ppp, 
             sigma=bw.diggle, 
             edge=TRUE, 
             kernel="gaussian"), 
     main="Gaussian")
plot(density(origins_SG_ppp, 
             sigma=bw.diggle, 
             edge=TRUE, 
             kernel="epanechnikov"), 
     main="Epanechnikov")
plot(density(origins_SG_ppp, 
             sigma=bw.diggle, 
             edge=TRUE, 
             kernel="quartic"), 
     main="Quartic")
plot(density(origins_SG_ppp, 
             sigma=bw.diggle, 
             edge=TRUE, 
             kernel="disc"), 
     main="Disc")

```

As they are identical, there is no need to choose a specific one. Moving forward, I will be using Gaussian kernel.

## Regions of Interest

We can tighten our analysis to include only areas of interest. From the figures above, we observe higher densities in the planning areas Changi, Jurong East, Woodlands and Marine Parade, we can filter out *mpsz* to get them.

```{r}
# extraction of Changi Airport
CA = mpsz_sf[mpsz_sf$PLN_AREA_N=="CHANGI",]

# extraction of Jurong East
JE = mpsz_sf[mpsz_sf$PLN_AREA_N=="JURONG EAST",]

# extraction of Woodlands
WL = mpsz_sf[mpsz_sf$PLN_AREA_N=="WOODLANDS",]

# extraction of Marine Parade
MP = mpsz_sf[mpsz_sf$PLN_AREA_N=='MARINE PARADE',]
```

With this, we now have to perform the same functions we did in *Create Owin Data* and *Conversions*.

```{r}
# turn them into spatial
CA_spatial <- as_Spatial(CA)
JE_spatial <- as_Spatial(JE)
WL_spatial <- as_Spatial(WL)
MP_spatial <- as_Spatial(MP)

# turn them into SpatialPolygons
CA_sp <- as(CA_spatial, "SpatialPolygons")
JE_sp <- as(JE_spatial, "SpatialPolygons")
WL_sp <- as(WL_spatial, "SpatialPolygons")
MP_sp <- as(MP_spatial, "SpatialPolygons")

# convert to owin

CA_owin = as(CA_sp, "owin")
JE_owin = as(JE_sp, "owin")
WL_owin = as(WL_sp, "owin")
MP_owin = as(MP_sp, "owin")
```

From here, we can extract the events that occurred in these planning areas.

```{r}
origins_CA_ppp = origins_SG_ppp[CA_owin]
origins_JE_ppp = origins_SG_ppp[JE_owin]
origins_WL_ppp = origins_SG_ppp[WL_owin]
origins_MP_ppp = origins_SG_ppp[MP_owin]
```

We can now perform the same analysis that we did in *Country Level Analysis*:

```{r}
kde_origins_CA <- density(origins_CA_ppp,
                              sigma=bw.diggle,
                              edge=TRUE,
                            kernel="gaussian") 
kde_origins_JE <- density(origins_JE_ppp,
                              sigma=bw.diggle,
                              edge=TRUE,
                            kernel="gaussian") 
kde_origins_WL <- density(origins_WL_ppp,
                              sigma=bw.diggle,
                              edge=TRUE,
                            kernel="gaussian") 
kde_origins_MP <- density(origins_MP_ppp,
                              sigma=bw.diggle,
                              edge=TRUE,
                            kernel="gaussian") 
```

We can also plot them out:

```{r}
par(mfrow=c(2,2))
plot(kde_origins_CA,
     main="Changi")
plot(kde_origins_WL,
     main="Woodlands")
plot(kde_origins_JE,
     main="Jurong East")
plot(kde_origins_MP,
     main="Marine Parade")
```

## Clark and Evans test

The Clark and Evans test allows us to understand if the data points are clustered in any places or not.

### Country Level Analysis

In the below code chunk, we are conducting this test on the origin points in the whole of Singapore. The hypothesis are as follows

H0: The Grab origin points are distributed equally throughout the country

H1: The Grab origin points have clusters.

```{r}
clarkevans.test(origins_SG_ppp,
                correction="none",
                clipregion="sg_owin",
                alternative=c("clustered"),
                nsim=99)
```

From this, we can see that there is clustering (R=0.28039) at a p-value of 2.2e-16. As 2.2e-16 is smaller than 0.05, we can conclude that this difference is significant at a 5% significance level. Hence, we reject the null hypothesis and the Grab origin points have clusters.

### Regions of Interest

#### Changi

```{r}
clarkevans.test(origins_CA_ppp,
                correction="none",
                clipregion=NULL,
                alternative=c("two.sided"),
                nsim=99)
```

From this, we can see that there is clustering (R= 0.31778) at a p-value of 2.2e-16. As 2.2e-16 is smaller than 0.05, we can conclude that this difference is significant at a 5% significance level. Hence, we reject the null hypothesis and the Grab origin points have clusters.

#### Woodlands

```{r}
clarkevans.test(origins_WL_ppp,
                correction="none",
                clipregion=NULL,
                alternative=c("two.sided"),
                nsim=99)
```

From this, we can see that there is clustering (R= 0.25797) at a p-value of 2.2e-16. As 2.2e-16 is smaller than 0.05, we can conclude that this difference is significant at a 5% significance level. Hence, we reject the null hypothesis and the Grab origin points have clusters.

#### Jurong East

```{r}
clarkevans.test(origins_JE_ppp,
                correction="none",
                clipregion=NULL,
                alternative=c("two.sided"),
                nsim=99)
```

From this, we can see that there is clustering (R= 0.25797) at a p-value of 2.2e-16. As 2.2e-16 is smaller than 0.05, we can conclude that this difference is significant at a 5% significance level. Hence, we reject the null hypothesis and the Grab origin points have clusters.

#### Marine Parade

```{r}
clarkevans.test(origins_MP_ppp,
                correction="none",
                clipregion=NULL,
                alternative=c("two.sided"),
                nsim=99)
```

From this, we can see that there is clustering (R= 0.51201) at a p-value of 2.2e-16. As 2.2e-16 is smaller than 0.05, we can conclude that this difference is significant at a 5% significance level. Hence, we reject the null hypothesis and the Grab origin points have clusters.

# Network Kernel Density Estimation

While the above does show the distribution of the points and which areas are more concentrated than others, they do not account for the network of the roads. In this section, we will be finding the networks (roads) that are more densely used as origins points in Changi Airport and Marina East.

## Data Handling

### Handling of Road Data

Now, we can read the road data as provided by [openstreetmap](https://www.openstreetmap.org/). Note that it provides data for Malaysia, Singapore and Brunei all at once.

```{r}
#| eval: False
all_roads <- st_read(dsn='../../data/data/data',
                     layer = 'gis_osm_roads_free_1')
```

We realise that it is in WGS 84, not in SVY21 so we have to transform the data.

```{r}
#| eval: False
all_roads <- st_transform(all_roads, 3414)
```

We can check the CRS again for all_roads.

```{r}
#| eval: False
st_crs(all_roads)
```

Since we're only interested in the roads in mainland Singapore, we have to filter the roads.

```{r}
#| eval: False
# get all roads in mainland Singapore
SG_roads <- st_intersection(all_roads, sg_sf)

# write out the roads into a separate .shp file for easier future handling
st_write(SG_roads, 'data/SG_roads.shp')
```

We can now read it from here.

```{r}
SG_roads <- st_read(dsn='data', layer= 'SG_roads')
```

### Getting the Roads within the Subzones

Now, we can find the streets that exist only inside the subzones.

```{r}
#| warnings: False
# get roads only in the aforementioned subzones
CA_roads <- st_intersection(SG_roads, CA)
WL_roads <- st_intersection(SG_roads, WL)
JE_roads <- st_intersection(SG_roads, JE)
MP_roads <- st_intersection(SG_roads, MP)
```

```{r}
CA_roads
WL_roads
JE_roads
MP_roads
```

We notice that all of the above are geometry type geometry and not a linestring. We can convert them with the following code chunk:

```{r}
#| warnings: False
CA_roads <- CA_roads %>%
  st_cast("LINESTRING")

WL_roads <- WL_roads %>%
  st_cast("LINESTRING")

JE_roads <- JE_roads %>%
  st_cast("LINESTRING")

MP_roads <- MP_roads %>%
  st_cast("LINESTRING")
```

### Extraction of the Events Within the Subzones

Before we can conduct analysis on the network, we will also need to constrict the events to exclusively the ones that occurred within these two subzones.

```{r}
#| warnings: False
#extraction of origin events that occurred within Changi and Marine Parade
CA_origins <- st_intersection(origins_sf, CA)
WL_origins <- st_intersection(origins_sf, WL)
JE_origins <- st_intersection(origins_sf, JE)
MP_origins <- st_intersection(origins_sf, MP)

```

We can now plot this data. In this instance, we can use Changi as an example:

```{r}
tmap_mode('view')
tm_shape(CA_origins)+
  tm_dots() +
  tm_shape(CA_roads) +
  tm_lines()
tmap_mode('plot')
```

## **Network Constrained KDE (NetKDE) Analysis**

### Preparing the lixel objects

Before computing NetKDE, the SpatialLines object need to be cut into lixels with a specified minimal distance. This task can be performed by using with *lixelize_lines()* of spNetwork as shown in the code chunk below.

```{r}
CA_lixels <- lixelize_lines(CA_roads, 
                         750, 
                         mindist = 375)
WL_lixels <- lixelize_lines(WL_roads, 
                         750, 
                         mindist = 375)
JE_lixels <- lixelize_lines(JE_roads, 
                         750, 
                         mindist = 375)
MP_lixels <- lixelize_lines(MP_roads, 
                         750, 
                         mindist = 375)
```

### Generating Line Points

Next, *lines_center()* of **spNetwork** will be used to generate a SpatialPointsDataFrame (i.e. samples) with line centre points as shown in the code chunk below.

```{r}
CA_samples <- lines_center(CA_lixels)
WL_samples <- lines_center(WL_lixels)
JE_samples <- lines_center(JE_lixels)
MP_samples <- lines_center(MP_lixels)
```

### Performing NetKDE

We can now compute the NetKDE using the code chunk below

```{r}
CA_densities <- nkde(CA_roads, 
                  events = CA_origins,
                  w = rep(1,nrow(CA_origins)),
                  samples = CA_samples,
                  kernel_name = "quartic",
                  bw = 8.080901, 
                  div= "bw", 
                  method = "simple", 
                  digits = 1, 
                  tol = 1,
                  grid_shape = c(1,1), 
                  max_depth = 8,
                  agg = 10, #we aggregate events within a 10m radius (faster calculation)
                  sparse = TRUE,
                  verbose = FALSE)
# care about bw (bandwith) and kernel_name 

JE_densities <- nkde(JE_roads, 
                  events = JE_origins,
                  w = rep(1,nrow(JE_origins)),
                  samples = JE_samples,
                  kernel_name = "quartic",
                  bw = 8.080901, 
                  div= "bw", 
                  method = "simple", 
                  digits = 1, 
                  tol = 1,
                  grid_shape = c(1,1), 
                  max_depth = 8,
                  agg = 10, #we aggregate events within a 10m radius (faster calculation)
                  sparse = TRUE,
                  verbose = FALSE)
 # care about bw (bandwith) and kernel_name 

WL_densities <- nkde(WL_roads, 
                  events = WL_origins,
                  w = rep(1,nrow(WL_origins)),
                  samples = WL_samples,
                  kernel_name = "quartic",
                  bw = 8.080901, 
                  div= "bw", 
                  method = "simple", 
                  digits = 1, 
                  tol = 1,
                  grid_shape = c(1,1), 
                  max_depth = 8,
                  agg = 10, #we aggregate events within a 10m radius (faster calculation)
                  sparse = TRUE,
                  verbose = FALSE)
# care about bw (bandwith) and kernel_name 

MP_densities <- nkde(MP_roads, 
                  events = MP_origins,
                  w = rep(1,nrow(MP_origins)),
                  samples = MP_samples,
                  kernel_name = "quartic",
                  bw = 8.080901, 
                  div= "bw", 
                  method = "simple", 
                  digits = 1, 
                  tol = 1,
                  grid_shape = c(1,1), 
                  max_depth = 8,
                  agg = 10, #we aggregate events within a 10m radius (faster calculation)
                  sparse = TRUE,
                  verbose = FALSE)
# care about bw (bandwith) and kernel_name 
```

### Visualising NetKDE

Before we can visualise the NetKDE values, code chunk below will be used to insert the computed density values (i.e. densities) into *samples* and *lixels* objects as *density* field.

```{r}
CA_samples$density <- CA_densities
CA_lixels$density <- CA_densities

JE_samples$density <- JE_densities
JE_lixels$density <- JE_densities

MP_samples$density <- MP_densities
MP_lixels$density <- MP_densities

WL_samples$density <- WL_densities
WL_lixels$density <- WL_densities
```

We can also rescale to make the values more understandable. Since the values are in kilometres, multiplying the values by 1000 will convert them to metres.

We can use the code chunk below to visualise the network kernel density estimation (using Changi as an example):

```{r}
tmap_mode('view')
tm_shape(CA_lixels)+
  tm_lines(col="density")+
tm_shape(CA_origins)+
  tm_dots()

tmap_mode('plot')
```
