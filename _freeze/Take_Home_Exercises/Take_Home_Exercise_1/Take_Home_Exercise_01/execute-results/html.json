{
  "hash": "4caa7f4421066ec4e98e918ec65f0a87",
  "result": {
    "markdown": "---\ntitle: \"Take Home Exercise 01: Part 1\"\nformat:\n  html:\n    code-fold: True\n    code-summary: \"Show the code\"\n    toc: True\n    toc-depth: 4\nexecute:\n  eval: True\n  echo: True\n  warning: False\n  freeze: True\ndate: \"2024-02-19\"\n---\n\n\n# Project Overview\n\nIt is important for local and government urban planners to know and understand how, where and when people move. For Singapore, data regarding human mobility has largely been collected and disseminated by the Land Transport Authority (LTA). However, they have only collected and publicised data regarding passenger volume from trains stations and bus stops. While immensely helpful, it still leaves out crucial information about human mobility through other means of transport, making it incomphrensive.\n\nIn 2020, Grab released Grab Posisi, a data set regarding the origin and destinations of Grab taxi users in Singapore.\n\nIn this take-home exercise, I will be finding the geographical and spatio-temporal distribution of Grab hailing service locations in Singapore.\n\nThe distribution will be analysed using the following, which will be displayed on the openstreetmap of Singapore:\n\n-   Traditional Kernel Density Estimation (KDE) layers: to analyse the where origin points tend to cluster\n\n-   Network Kernel Density Estimation (NKDE): to analyse the clusters of origin points, as constrained by the road.\n\n# Section Overview\n\nIn this section, I will be performing the EDA and data handling. Part 2 will be dealing with the traditional kernel density estimation and network kernel density estimation.\n\n# The Data\n\n| Name                 | Description                                                | Format   | Source                                                                                   |\n|----------------------|------------------------------------------------------------|----------|------------------------------------------------------------------------------------------|\n| MP14_SUBZONE_WEB_PL  | Subzone Boundary of Singapore (2014)                       | .shp     | [data.gov.sg](https://beta.data.gov.sg/datasets/d_d14da225fccf921049ab64238ff473d9/view) |\n| Grab Posisi          | Dataframe of Origin and Destination Locations through Grab | .parquet | Provided by professor                                                                    |\n| gis_osm_roads_free_1 | Road network of Malaysia, Singapore and Brunei             | .shp     | [geofabrik.de](https://download.geofabrik.de/asia/malaysia-singapore-brunei.html)        |\n\n: Data Sets Used\n\n# Loading the packages\n\nIn this section, I will be loading the following packages:\n\n-   maptools\n\n-   sf\n\n-   raster\n\n-   spatstat\n\n-   tmap\n\n-   tidyverse\n\n-   arrow\n\n-   lubridate\n\n-   spNetwork\n\n-   classInt\n\n-   viridis\n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(maptools, sf, raster, spatstat, tmap, tidyverse, arrow, lubridate, spNetwork, classInt)\n```\n:::\n\n\n# General Handling\n\n## Extraction of the Coastal Outline of Mainland Singapore\n\nFor the purposes of this analysis, we will also need to extract the Coastal Outline of Singapore. This is to create a boundary for all the points to fall under.\n\nFirst, we need to read Singapore Master Plan Subzone 2014 to get Singapore's shape overall.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmpsz_sf <- st_read(dsn = \"../../data/data\",\n                layer = \"MP14_SUBZONE_WEB_PL\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `D:\\KrisLBT\\IS415-GAA\\data\\data' using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n```\n:::\n:::\n\n\nWe have to check the CRS to ensure that it is in the correct coordinates system.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nst_crs(mpsz_sf)\n```\n:::\n\n\nSince it is in the wrong CRS (WGS84), we have to correct it to SVY21.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmpsz_sf <- st_transform(mpsz_sf, 3414)\n```\n:::\n\n\nNow, we can plot the island.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(mpsz_sf)\n```\n:::\n\n\nWe notice that the planning subzone includes areas where Grab does not serve due to lack of bridges. Hence, any roads in unavailable areas would either not be useful or actually distort the network analysis.\n\nMost of the areas unserved by Grab are the outer islands. For this, we can identify which of the outer islands to exclude from our analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nouter_islands<-  filter(mpsz_sf, grepl('ISLAND', PLN_AREA_N))\nplot(outer_islands[\"SUBZONE_N\"])\n```\n:::\n\n\nHowever, we realise that not all the islands are unavailable to Grab drivers. Grab drivers are able to pick up and drop off at [Sentosa](https://www.sentosa.gov.sg/files/resources/news/20191121_media_release_sdc_grab_partnership.pdf).\n\nSince 2019, Grab drivers have been allowed entry to [Jurong Island](https://www.grab.com/sg/driver/transport/jurong-island-security-pass-enrolment/), provided they have a security pass. However, given that very few drivers are allowed entry and the fact that this region is generally inaccessible to the public, we will be excluding them.\n\nHence, the islands we need to exclude are:\n\n-   Semakau\n-   Southern Group\n-   Sudong\n-   North-eastern Islands\n-   Jurong Island and Bukom\n\n\n::: {.cell}\n\n```{.r .cell-code}\nserviced_area <- mpsz_sf %>%\n  filter(!grepl(\"SEMAKAU|SOUTHERN GROUP|SUDONG|NORTH-EASTERN ISLANDS|JURONG ISLAND AND BUKOM\", SUBZONE_N)) \nplot(serviced_area)\n```\n:::\n\n\nNow, we can blend away the boundaries.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsg_sf <- serviced_area %>%\n  st_union()\n```\n:::\n\n\nPlot the sg_sf and we now see we have the CoastalOutline of Singapore that is served by Grab.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(sg_sf)\n```\n:::\n\n\nWe can save it as the CoastalOutline as a shapefile and read it again\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# writing the CoastalOutline\nst_write(sg_sf, 'data/CoastalOutline.shp')\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# reading the CoastalOutline\nsg_sf <- st_read(dsn='data/',\n                 layer = \"CoastalOutline\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nReading layer `CoastalOutline' from data source \n  `D:\\KrisLBT\\IS415-GAA\\Take_Home_Exercises\\Take_Home_Exercise_1\\data' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1 feature and 1 field\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 21494.3 xmax: 55941.94 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\n```\n:::\n:::\n\n\n## Handling of the Grab Posisi data\n\nWe will also have to handle the Grab Posisi data.\n\nThe data is quite large. As such, Grab has divided into 10 parquet files for easier distribution.\n\n### Reading the Grab Posisi Data\n\nI will now be loading the Grab Posisi data\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf <- read_parquet(file=\"../../data/data/GrabPosisi/part-00000.snappy.parquet\")\ndf1 <- read_parquet(file=\"../../data/data/GrabPosisi/part-00001.snappy.parquet\")\ndf2 <- read_parquet(file=\"../../data/data/GrabPosisi/part-00002.snappy.parquet\")\ndf3 <- read_parquet(file=\"../../data/data/GrabPosisi/part-00003.snappy.parquet\")\ndf4 <- read_parquet(file=\"../../data/data/GrabPosisi/part-00004.snappy.parquet\")\ndf5 <- read_parquet(file=\"../../data/data/GrabPosisi/part-00005.snappy.parquet\")\ndf6 <- read_parquet(file=\"../../data/data/GrabPosisi/part-00006.snappy.parquet\")\ndf7 <- read_parquet(file=\"../../data/data/GrabPosisi/part-00007.snappy.parquet\")\ndf8 <- read_parquet(file=\"../../data/data/GrabPosisi/part-00008.snappy.parquet\")\ndf9 <- read_parquet(file=\"../../data/data/GrabPosisi/part-00009.snappy.parquet\")\n```\n:::\n\n\n### Handling the Grab Posisi data\n\n#### Time stamp\n\nUpon investigating the file, I noticed that there was supposed to be a datetime stamp but it wasn't in that format. As such, the data type needs to be changed.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf$pingtimestamp <- as_datetime(df$pingtimestamp)\ndf1$pingtimestamp <- as_datetime(df1$pingtimestamp)\ndf2$pingtimestamp <- as_datetime(df2$pingtimestamp)\ndf3$pingtimestamp <- as_datetime(df3$pingtimestamp)\ndf4$pingtimestamp <- as_datetime(df4$pingtimestamp)\ndf5$pingtimestamp <- as_datetime(df5$pingtimestamp)\ndf6$pingtimestamp <- as_datetime(df6$pingtimestamp)\ndf7$pingtimestamp <- as_datetime(df7$pingtimestamp)\ndf8$pingtimestamp <- as_datetime(df8$pingtimestamp)\ndf9$pingtimestamp <- as_datetime(df9$pingtimestamp)\n```\n:::\n\n\n#### Merging them into 1 data frame\n\nI also noticed that the the same ride might actually have origin and destination points through multiple data frames. As such, we need to merge them before we can extract the origin points.\n\n\n::: {.cell}\n\n```{.r .cell-code}\norigin_df_all <- df %>%\n  rbind(df1) %>%\n  rbind(df2) %>%\n  rbind(df3) %>%\n  rbind(df4) %>%\n  rbind(df5) %>%\n  rbind(df6) %>%\n  rbind(df7) %>%\n  rbind(df8) %>%\n  rbind(df9)\n```\n:::\n\n\n#### Extraction of Origin\n\nNow, I will be extracting the origin.\n\nThe way I did it below is by performing the following steps\n\n-   arrange it according to the trj_id (unique id determining where the person wants to at that point)\n-   arranging it according to the time stamp (beginning and end)\n-   getting the first row - mutating the data to weekday, start hour and the day\n\n\n::: {.cell}\n\n```{.r .cell-code}\norigin_df <- origin_df_all %>%\n  group_by(trj_id) %>%\n  arrange(pingtimestamp) %>%\n  filter(row_number()==1) %>%\n  mutate(weekday = wday(pingtimestamp,\n                        label=TRUE,\n                        abbr=TRUE),\n         start_hr= factor(hour(pingtimestamp)),\n         day = factor(mday(pingtimestamp)))\n```\n:::\n\n\n##### Writing of Origin Data to RDS\n\nNow, I will write the origin data to rds for easier future handling.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwrite_rds(origin_df, \"data/rds/origin_dfs.rds\")\n```\n:::\n\n\n##### Reading of RDS data\n\nNow, we can clear the data in our environment and just read the rds data we saved.\n\n\n::: {.cell}\n\n```{.r .cell-code}\norigin_df <- read_rds(\"data/rds/origin_dfs.rds\")\n```\n:::\n\n\n## Exploratory Data Analysis and Handling of Origins\n\nIn this section, I will be exploring the general distribution of the origin points and handling it for the subsequent sections, which will analyse the data in greater detail.\n\n### Visualising the Frequency Distribution\n\nFirst, I will plot out the general distribution of the origin points\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data=origin_df,\n       aes(x=weekday)) +\n  geom_bar()\n```\n\n::: {.cell-output-display}\n![](Take_Home_Exercise_01_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\nAs can be seen, the number of trips daily seem to be fairly equally distributed throughout the week.\n\n### Visualising the Origins as a Point Map Symbol\n\nWe can visualise the geospatial distribution of the origin points. First, we need to convert the coordinates to lat-long and transform it to the correct CRS.\n\n\n::: {.cell}\n\n```{.r .cell-code}\norigins_sf <- st_as_sf(origin_df, coords = c(\"rawlng\", \"rawlat\"),\n                       crs=4326) %>%\n  st_transform(crs=3414)\n```\n:::\n\n\nNow, we can visualise it using the code chunk below:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntm_shape(mpsz_sf)+\n  tm_polygons()+\ntm_shape(origins_sf) +\n  tm_dots()\n```\n:::\n\n\n### Handling\n\nFor further analysis, we will need to convert the various data into different formats. These will be explained and performed in the code chunk below\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#conversion of sg_sf, mpsz_sf and origins_sf to a generic spatial class\nsg <- as_Spatial(sg_sf)\nmpsz <- as_Spatial(mpsz_sf)\norigins <- as_Spatial(origins_sf) \n\n# Conversion of sg and origins to SpatialPolygon format\nsg_sp <- as(sg, \"SpatialPolygons\")\norigins_sp <- as(origins, \"SpatialPoints\")\n\n# conversion of origins to ppp format\norigins_ppp <- as(origins_sp, \"ppp\")\norigins_ppp\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPlanar point pattern: 28000 points\nwindow: rectangle = [3628.24, 49845.23] x [25198.14, 49689.64] units\n```\n:::\n:::\n\n\nNow, I will plot origins_ppp.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(origins_ppp)\n```\n:::\n\n\nWe must check for duplicates in the data using the code chunk below:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nany(duplicated(origins_ppp))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] FALSE\n```\n:::\n:::\n\n\nAs there are no duplicates within the data, we do not have to apply any more transformation to the data.\n\n#### Create Owin Data\n\nNow, we can convert the Coastal Outline to owin data and plot it out.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsg_owin <- as(sg_sp, \"owin\")\nplot(sg_owin)\n```\n\n::: {.cell-output-display}\n![](Take_Home_Exercise_01_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n\n#### Combining Events\n\nWe will now extract Grab origins within the Singapore CoastalOutline\n\n\n::: {.cell}\n\n```{.r .cell-code}\norigins_SG_ppp = origins_ppp[sg_owin]\n\n# plotting the distribution of the origins\nplot(origins_SG_ppp)\n```\n\n::: {.cell-output-display}\n![](Take_Home_Exercise_01_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "Take_Home_Exercise_01_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}