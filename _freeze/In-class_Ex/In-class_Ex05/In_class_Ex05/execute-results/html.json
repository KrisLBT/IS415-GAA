{
  "hash": "d511ecfab1d0e129bb2c870ad9290f91",
  "result": {
    "markdown": "---\ntitle: \"In-class Exercise 05 - Global and Local Measures of Spatial Association: sfdep method\"\nformat:\n  html:\n    code-fold: True\n    code-summary: \"Show the code\"\n    toc: True\n    toc-depth: 4\nexecute:\n  eval: True\n  echo: True\n  warning: False\n  freeze: True\ndate: \"2024-02-19\"\n---\n\n\n# Installation of Packages\n\nFor this in-class exercise, we will be using the following packages and loading it into our R environment:\n\n-   sf\n-   tmap\n-   sfdep\n    -   the newer version of spdep;\n    -   can use immediately with sf objects; will save to sf layer) -- old version had to extract it out\n-   tidyverse\n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(sf, tmap, sfdep, tidyverse)\n```\n:::\n\n\n# Loading the Data\n\nFor the purpose of our analysis, we will be using the following datasets:\n\n-   Hunan, a geospatial data set in ESRI shapefile format, and\n-   Hunan_2012, an attribute data set in csv format\n\n## Importing Geospatial Data\n\nFirst, I will be importing *Hunan*.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nHunan <- st_read(dsn=\"data/geospatial\",\n                 layer = \"Hunan\") \n```\n\n::: {.cell-output .cell-output-stdout}\n```\nReading layer `Hunan' from data source \n  `D:\\KrisLBT\\IS415-GAA\\In-class_Ex\\In-class_Ex05\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 88 features and 7 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 108.7831 ymin: 24.6342 xmax: 114.2544 ymax: 30.12812\nGeodetic CRS:  WGS 84\n```\n:::\n:::\n\n\n::: callout-note\n## Note\n\nCheck if the bounding box is in decimal deree or not. If it's in decimal degree, it is in geodetic CRS, not projected CRS.\n:::\n\n## Importing Aspatial Data\n\nNow, I will be importing *Hunan_2012.csv* into my R environment\n\n\n::: {.cell}\n\n```{.r .cell-code}\nHunan_2012 = read_csv(\"data/aspatial/Hunan_2012.csv\")\n```\n:::\n\n\n## Joining the Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# will lose geometry for sf layer\nhunan_GDPPC <- left_join(Hunan, Hunan_2012) %>%\n  select(1:4, 7, 15)\n```\n:::\n\n\n::: Callout-note\n## Notes\n\nCheck if anything can be used to join the data frames (geospatial and aspatial). This can be County name etc.\n\nAlso, perform left_join to prevent losing the geometric data\n:::\n\n# Plotting the Choropleth Map\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntmap_mode(\"plot\")\ntm_shape(hunan_GDPPC) +\n  tm_fill(col=\"GDPPC\",\n         style = \"quantile\",\n         palette= \"Blues\",\n         title = \"GDPPC\") +\n  tm_layout(main.title = \"Distribution of GDP per capita by region\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45,\n            legend.width = 0.35,\n            frame= TRUE) +\n  tm_borders(alpha=0.5) +\n  tm_compass(type=\"8star\", size=2) +\n  tm_scale_bar() +\n  tm_grid(alpha=0.2)\n```\n\n::: {.cell-output-display}\n![](In_class_Ex05_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n# Step 1: Deriving Contiguity Weights\n\n## Queens's Method\n\nYou can go directly into the Queen's Method using tidyverse in a singular step:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwm_q <- hunan_GDPPC %>%\n  mutate(nb=st_contiguity(geometry),\n         wt= st_weights(nb,\n                        style = \"W\"),\n         .before =1)\n```\n:::\n\n\n::: Callout-note\n### Note\n\n.before = 1 means to ensure one column will always be the first May not be necessary in real life applications\n:::\n\n## Computing Global Moran I\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmoranI <- global_moran(wm_q$GDPPC,\n                      wm_q$nb,\n                      wm_q$wt)\nglimpse(moranI)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nList of 2\n $ I: num 0.301\n $ K: num 7.64\n```\n:::\n:::\n\n\nHowever, Global Moran I might not be convincing enough\n\n## Performing Global Moran I permutation test\n\nCan do it on multiple simulations (in this case 100. This is because the number of simulations is nsim+1) to confirm the representativeness of the data set.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglobal_moran_perm(wm_q$GDPPC,\n                  wm_q$nb,\n                  wm_q$wt,\n                  nsim = 99)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tMonte-Carlo simulation of Moran I\n\ndata:  x \nweights: listw  \nnumber of simulations + 1: 100 \n\nstatistic = 0.30075, observed rank = 100, p-value < 2.2e-16\nalternative hypothesis: two.sided\n```\n:::\n:::\n\n\nThe statistical report above demonstrates that the p-value is smaller than 0.05, which means that this result is significant at a 95% significance level. Hence, we can reject the null hypothesis that the spatial distribution of GDP per capita resembles random distribution. Because the Moran's I value is greater than 0, we can infer that the spatial distribution shows signs of clustering.\n\n::: Callout-note\n### Note\n\nnsim will effect computation time. If you're on a larger data set, you can just do a smaller number (e.g. 99, 49). 999 may take an incredibly long time to load. Test if your compute can handle it within a certain timeframe.\n\nWe can also only infer from the results, we can't definitively *prove/disprove* our hypothesis\n:::\n\nComplete in-class exercise 5 on our own + watch video for hot and cold spot\n",
    "supporting": [
      "In_class_Ex05_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}